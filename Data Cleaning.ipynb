{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a54a48e0",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "---\n",
    "Prepared by Natalie Castro  \n",
    "CSCI 5832\n",
    "\n",
    "---\n",
    "\n",
    "The purpose of this notebook is to clean the Kaggle Sentiment140 dataset for masked language modeling with the task of sentiment prediction. The dataset may be retrieved at [Kaggle's Website](https://www.kaggle.com/datasets/kazanova/sentiment140/data) The dataset's original use was provided for Distance Learning in 2009 from a team at Stanford (Go, Bhayani, Huang: 2009). Sentiment was then defined as a \"personal positive or negative feeling\". The labels employed for subsequent training were preserved from the original dataset. These labels were generated from 'Twittratr', a website used to generate Tweet analysis with a balanced list of keywords. In addition, the feature reduction techniques presented by the dataset curators were continued in this analysis\n",
    "\n",
    "**Dataset Characteristics:**\n",
    "* Emojis were stripped from the data - (Go et al: 2009) provided rationale for this as the classifier would then learn more from the unigram or bigram sets of emoji use instead of the actual meaning of the Tweet - they are \"noisy\" labels.\n",
    "\n",
    "* Usernames were replaced with a [username] token. This is to avoid any potential sentiment biases based on the user who is mentioned. \n",
    "\n",
    "* URLs were replaced with a [URL] token. This is to ensure that lengthy URLs do not cause for incorrect padding \n",
    "\n",
    "* Repeated letters were removed with Regex techniques. The example provided by (Go et al: 2009) illustrated the following example: \"huuuungry, huuuuungry, and huuuuuuuuuuungry\". Any additional 'u' tokens were replaced with only two occurances. \n",
    "\n",
    "**Masking Emotionally Charged Words:**\n",
    "Twittratr had generated a list of positive and negative words. While not available on Twittratr's current website, these lists were preserved on the Internet Archive. The [positive words](https://web.archive.org/web/20090716125551/http://docs.google.com/Doc?id=df5m8zwp_92gvtfm3d9) and [negative words](https://web.archive.org/web/20090709213341/http://docs.google.com/Doc?id=df5m8zwp_93gd2mhkd7) are linked both here and saved in the project's repository. \n",
    "\n",
    "Particular words related to these sentiments will be intentionally masked, at random distributions, in an attempt to increase the preformance of the sentiment classifier. Devlin and colleagues (2019) utilized masked language modeling (also known as the *Cloze* task) to deepen the learning of the model. 15% of the tokens in the input were masked. The following distribution was then followed: 80% the token was replaced with [MASK], 10% of the time the token is replaced with a randomly sampled token from the unigram distribution (Jurafsky and Martin: 2025). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd531778",
   "metadata": {},
   "source": [
    "## 1. Environment Creation\n",
    "\n",
    "### 1.1 Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "399ac5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' DATA MANAGEMENT '''\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "''' DATA PARSING '''\n",
    "import regex as re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "''' DATA MASKING '''\n",
    "import random\n",
    "\n",
    "''' DATA VISUALIZATION '''\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ab4e53",
   "metadata": {},
   "source": [
    "### 1.2 Dataset Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59600ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Sentiment140 Twitter Data.csv\", encoding='latin-1', names = ['sentiment','tweet_id','date','flag','user','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72bfc229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment    tweet_id                          date      flag  \\\n",
       "0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2          0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3          0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4          0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6d00ee",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a91c412",
   "metadata": {},
   "source": [
    "### 2.1 Sentiment Labeling\n",
    "\n",
    "A few different sources online illustrate the ambiguity of the labels - a more illustrative label will be generated for the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "968c2802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 4}\n"
     ]
    }
   ],
   "source": [
    "''' EXPLORING THE SENTIMENT VALUES  '''\n",
    "\n",
    "print (set(data['sentiment'].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01eea529",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' REMAPPING VALUES '''\n",
    "\n",
    "## Creating a function to use in a lambda apply \n",
    "def label_mapper(x):\n",
    "    if x == 0:\n",
    "        return (\"Negative\")\n",
    "    \n",
    "    else:\n",
    "        return (\"Positive\")\n",
    "    \n",
    "\n",
    "## Applying the function to the dataframe and generating a sentiment label column\n",
    "data['sentiment label'] = data['sentiment'].copy().apply(lambda x: label_mapper(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48e7af6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment    tweet_id                          date      flag  \\\n",
       "0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2          0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3          0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4          0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \\\n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...   \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...   \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire    \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "  sentiment label  \n",
       "0        Negative  \n",
       "1        Negative  \n",
       "2        Negative  \n",
       "3        Negative  \n",
       "4        Negative  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fe608fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = data.at[0, 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9afb5424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a5e1bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text2 = \"[USER] He did!   I watched it last night.  Whataaaaa jerk.  I'm sooo not rooting for him anymore!!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75defb1",
   "metadata": {},
   "source": [
    "### 2.2 Token Removal\n",
    "\n",
    "#### 2.2.1 User Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8919f266",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' REPLACING ANY USER TAG WITH [USER] '''\n",
    "\n",
    "## Defining a RegEx pattern\n",
    "## This pattern first captures the '@' symbol and any (non)alphanumeric tokens to the next white space\n",
    "user_pattern = r\"(?:@)\\w*\\W*(?=\\s)\"\n",
    "user_token = '[USER]'\n",
    "\n",
    "## Creating a function intended to be used with a lambda apply\n",
    "def user_tags(x):\n",
    "    cleaned_token = re.sub(user_pattern, user_token, x)\n",
    "    return (cleaned_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df9fd421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[USER] http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_tags(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b10972e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using lambda apply to replace the 'text' column\n",
    "\n",
    "data['text'] = data['text'].apply(lambda x: user_tags(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59e3c4d",
   "metadata": {},
   "source": [
    "#### 2.2.2 URL Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "437f826b",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' REPLACING ANY URL TOKEN WITH [URL]'''\n",
    "\n",
    "## Defining a RegEx pattern\n",
    "## This pattern identifies any https, http, or any other prefix before a :// indicative of a URL\n",
    "## Next, it non-greedily searches for any character until the next white space or the end of the string\n",
    "\n",
    "## The second URL pattern searches for any URLs that may be mentioned by name, but not actual hyperlink.\n",
    "url_pattern1 = r\"\\w+://.+?(?=\\s|\\Z)\"\n",
    "url_pattern2 = r\"\\S+?(\\.com|\\.edu|\\.org|\\.net|\\.gov|\\.io|\\.co|\\.uk|\\.ca|\\.de)(?=\\s|\\Z)\"\n",
    "url_token = '[URL]'\n",
    "\n",
    "## Creating a function that replaces any URLs with the defined token\n",
    "def url_tags(x):\n",
    "    cleaned_token = re.sub(url_pattern1, url_token, x)\n",
    "    cleaned_token = re.sub(url_pattern2, url_token, cleaned_token)\n",
    "    return (cleaned_token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4933682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[USER] nawong How do I go about getting access to [URL] I'm dying to try it out\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_tags(\"[USER] nawong How do I go about getting access to idzr.org I'm dying to try it out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03c22531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@switchfoot [URL] - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_tags(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5429177e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning the text column\n",
    "\n",
    "data['text'] = data['text'].apply(lambda x: url_tags(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e059452",
   "metadata": {},
   "source": [
    "#### 2.2.3 Multiple Character Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbe9f9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' REPLACING MULTIPLE CHARACTERS WITH ONLY TWO '''\n",
    "\n",
    "## Defining a RegEx pattern\n",
    "## This pattern identifies any alphanumeric character then 'looks back' twice.\n",
    "## This pattern will match any consecutive strings of the same number or letter and 'leave' the last two\n",
    "## Because of this, a RegEx sub for \"\\1\\1\" will remove any more than two characters.\n",
    "multiple_pattern = r\"(\\w)\\1{2,}\"\n",
    "multiple_token = r'\\1\\1'\n",
    "\n",
    "## Creating a function that replaces any multiple characters with the defined token\n",
    "def character_multiplier(x):\n",
    "    cleaned_token = re.sub(multiple_pattern, multiple_token, x)\n",
    "    return (cleaned_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2beb8968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@switchfoot http://twitpic.com/2y1zl - Aww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_multiplier(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36740d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning the text column \n",
    "\n",
    "data['text'] = data['text'].apply(lambda x: character_multiplier(x))"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "5b6727a2",
   "metadata": {},
   "source": [
    "#### 2.2.4 Emoji Removal\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "*Citation: Go, Bhayani, Huang: 2009*\n",
    "\n",
    "Deviating from Go and Colleagues, the emojis will be removed entirely from this dataset. The rationale behind this is driven mainly by the age of the emojis utilized. While in many cases the ':)' emojis are still used, the ubiquity of emojis, emoji combinations, and development over the years - they may mean something completely different than in early social media context. In an attempt to 'translate' the temporal capabilities of the dataset, the emojis were removed. \n",
    "\n",
    "Sequentially, removing emojis *after* removing the URLs is important because the ReGEx may pick up on the colon character used in URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d50f62e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' REMOVING EMOJIS '''\n",
    " \n",
    "## The below pattern is instantiated to remove any emojis in the above table, in addition\n",
    "## to their winking counterparts. \n",
    "emoji_pattern = r\"(:|;\\))|(:|;-\\))|(:|; \\))|(:|;D)|(=\\))|(:|;\\))|(:|;\\()|(:|; \\()\"\n",
    "emoji_token = \"\"\n",
    "\n",
    "## Creating a function to remove the emojis\n",
    "def emoji_remover(x):\n",
    "    cleaned_token = re.sub(emoji_pattern, emoji_token, x)\n",
    "    return (cleaned_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2deb6da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@switchfoot http//twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. \""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji_remover(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff0757d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Applying the remover to the entire columns\n",
    "\n",
    "data['text'] = data['text'].apply(lambda x: emoji_remover(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1a76bd",
   "metadata": {},
   "source": [
    "### 2.3 Examining Cleaned Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9bc1af24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the original dataset is 1600000.\n",
      "\n",
      "The length of the de-duplicated dataset is 1556582.\n"
     ]
    }
   ],
   "source": [
    "''' REMOVING DUPLICATES WITH A SUBSET OF TWEET AND SENTIMENT '''\n",
    "\n",
    "print (f\"The length of the original dataset is {len(data)}.\\n\")\n",
    "data = data.drop_duplicates(subset=['sentiment label', 'text'])\n",
    "\n",
    "print (f\"The length of the de-duplicated dataset is {len(data)}.\")\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cebbf6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = data['text'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "670840b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['just gotten out of bed, I hope I start feeling some life in the old girl soon, gonna go and tackle my 16 year olds bedroom in a mo  smell',\n",
       " 'auuww my mouth is hurting ',\n",
       " 'just had a mam as talk wif dad bout...shit. [USER] still underhouse arrest. BASICALLY ALL MY TRUST=GONE ',\n",
       " 'My throat feels like I swallowed a hot coal. ',\n",
       " \"[USER] yeah, I wasn't expecting that at all  I doubt that's the last we'll hear of it though\",\n",
       " \"Why do my Sprint phones always encounter problems like this around the 6 - 7 month mark??? I'm locked into a 50 yr contract at this point \",\n",
       " 'My coffee was cold ',\n",
       " \"Wow, a high of 64 today. I love this weather in Florida. Sadly it won't last long \",\n",
       " 'Have research report due at five  in the library trying to get it finished',\n",
       " \"[USER] i'm guessing that was to me, cause these chairs be a pain, but i have no spare powerpoints in my room \",\n",
       " \"Damn, all my joints are sore at the moment. Either I'm feeling the effects of my PT session, or I'm getting sick, or both. \",\n",
       " 'There is pollen all over my car but it does NOT feel like spring. ',\n",
       " \"Eww  just woke up and now I'm about to start tonights hw...... please someone save me??! Hehe\",\n",
       " 'is sick and cannot go to work   Will see everyone tomorrow',\n",
       " \"libtorrent.... why can't you just work and be nice and small at the same time? \",\n",
       " 'i could not update at all yesterday ',\n",
       " 'Just finished work tryin 2 find something 2 do  x',\n",
       " \"is [USER] St Mary's  [URL]\",\n",
       " 'Yet another designer gives me webpage designs in PSD (PhotoShop) format. ',\n",
       " 'fighting a bad swollen, painful throat and headache. ']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[6300:6320]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aeccecb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Snow mounted up to a couple of inches overnight, very pretty but I hope I don't lose my peaches, tree was in bloom \",\n",
       " \"[USER] Still don't have the confirmation email  Technically I could give up already \",\n",
       " '[USER] A tenner?! What a bloody cop out! ',\n",
       " '[USER] [URL] - i miss your old hairstyle daniel  but you still look great',\n",
       " 'Headed 2 a performance...outside...n the rain.  If I do the rain dance backwards will that make it stop? =\\\\',\n",
       " \"i wanna get 93841934431984139418349134 dvd's... i hope there is a good dvd store here. im a dvdaholic... and an alcoholic \",\n",
       " \"i feel ridiculous. and i'm blaming you.  WHY HJSDKFSDHFJSD\",\n",
       " \"everyone is talking about britney's concerts... i want to go \",\n",
       " \"*yawn* g'mornin' have headache.  pls depart! veggin' w/cuppa chai b4 I have to wrap my head around the day.\",\n",
       " 'Listening to mariah carey we belong together ']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[10000:10010]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f0afd0",
   "metadata": {},
   "source": [
    "### 2.4 Removing any Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627b3206",
   "metadata": {},
   "source": [
    "## 3. Masking\n",
    "\n",
    "A reminder of the stated masking protocol above:\n",
    "\n",
    "Devlin and colleagues (2019) utilized masked language modeling (also known as the Cloze task) to deepen the learning of the model. 15% of the tokens in the input were masked. The following distribution was then followed: 80% the token was replaced with [MASK], 20% of the time the token is replaced with a randomly sampled token from the unigram distribution (Jurafsky and Martin: 2025)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173829ff",
   "metadata": {},
   "source": [
    "### 3.1 Selecting Data to Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee9244e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL TWEETS: 1556582\n",
      "\n",
      "MASKED TWEETS: 233487\n",
      "\n",
      "Out of the Masked Tweets...\n",
      "\tMASK TOKEN: 186790\n",
      "\tRANDOM TOKEN AMOUNT: 46697\n"
     ]
    }
   ],
   "source": [
    "''' REPORTING ON THE PROPORTIONS OF DATA THAT ARE MASKED '''\n",
    "\n",
    "dataset_length = len(data)\n",
    "\n",
    "tokens_to_mask = round(dataset_length * 0.15)\n",
    "\n",
    "mask_token_amount = round(tokens_to_mask * 0.80)\n",
    "\n",
    "random_token_amount = round(tokens_to_mask * 0.20)\n",
    "\n",
    "print (f\"TOTAL TWEETS: {dataset_length}\\n\\nMASKED TWEETS: {tokens_to_mask}\\n\\nOut of the Masked Tweets...\\n\\tMASK TOKEN: {mask_token_amount}\\n\\tRANDOM TOKEN AMOUNT: {random_token_amount}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5eaa32d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' SELECTING 233,579 RANDOM TWEETS '''\n",
    "\n",
    "## Setting the entire population to be the range of the dataset, \n",
    "masked_indicies = random.sample(range(dataset_length), tokens_to_mask)\n",
    "\n",
    "## Creating a list of indicies that are to remain the same\n",
    "all_indicies = range(dataset_length)\n",
    "unmasked_indicies = list(set(all_indicies) - set(masked_indicies))\n",
    "\n",
    "\n",
    "## Filtering the data for masking\n",
    "masked_data = data.iloc[masked_indicies].copy()\n",
    "unmasked_data = data.iloc[unmasked_indicies].copy()\n",
    "\n",
    "## Reseting the dataframe's indicies in place so any sort of .at or \n",
    "## query functions will work subsequently\n",
    "masked_data.reset_index(inplace=True, drop=True)\n",
    "unmasked_data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "788f92a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' SELECTING 186,863 TWEETS TO MASK'''\n",
    "\n",
    "mask_token = '[MASK]'\n",
    "\n",
    "## Generating random indicies from the masked parition\n",
    "masked_token_indicies = random.sample(range(tokens_to_mask), mask_token_amount)\n",
    "\n",
    "## Filtering the data for other type of masking\n",
    "mask_idx = range(mask_token_amount)\n",
    "random_indicies = list(set(mask_idx) - set(range(random_token_amount)))\n",
    "\n",
    "## Creating a random replacement dataframe and a mask token replacement frame\n",
    "masked_tokens_data = masked_data.iloc[masked_token_indicies].copy()\n",
    "random_tokens_data = masked_data.iloc[random_indicies].copy()\n",
    "\n",
    "## Resetting the dataframe's indicies\n",
    "masked_tokens_data.reset_index(inplace=True, drop=True)\n",
    "random_tokens_data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8cac84",
   "metadata": {},
   "source": [
    "### 3.2 Replacing Tokens with Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dec91183",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' CREATING A RANDOM MASKER WITH RESPECT TO TWEET LENGTH '''\n",
    "\n",
    "## Creating a RegEx match pattern to split on whitespace\n",
    "word_pattern = r\"\\S+\"\n",
    "def masker(x):\n",
    "    ## Using RegEx findall to generate a list of the words from the tweet\n",
    "    words = re.findall(word_pattern,x)\n",
    "   \n",
    "    ## Identifying the length of the tweet\n",
    "    tweet_length = len(words)\n",
    "    \n",
    "    ## Selecting a random token to mask from the length of the tweet\n",
    "    token_number = random.sample(range(tweet_length),1)\n",
    "    \n",
    "    words[token_number[0]] = '[MASK]'\n",
    "    \n",
    "    ## Rejoining the list to form a tweet\n",
    "    tweet = ' '.join(words)\n",
    "    \n",
    "    return (tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c0c8c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hrm, scanner traffic diminishes greatly when [MASK] public services are on strike.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masker('Hrm, scanner traffic diminishes greatly when your public services are on strike. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8788bc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_tokens_data['masked text'] = masked_tokens_data['text'].apply(lambda x: masker(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56226ab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['At what point in my life did I have to start seriously asking how I get myself checked in? [MASK] I know someone I could ask',\n",
       " 'I hate [MASK] previews lie.',\n",
       " '[MASK] think youre being unfair nathan',\n",
       " '[USER] eis [MASK]',\n",
       " 'LVATT is a trending topic!? Nice Im getting it as soon as target opens, which is at 8 ( Then off to regents [MASK] 9.',\n",
       " '[USER] glad [MASK] agree love',\n",
       " '[USER] Ah, you just [MASK] to be signed in to view the other one!',\n",
       " 'just remembered the second reason [MASK] she hated the sun/heat',\n",
       " 'HeadeD 2 tHe maLL 4 soMe odd reaSon....2 find soMething [MASK]',\n",
       " 'has just woke up.. what a [MASK] im gettin too old for this']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' EXAMINING A FEW OF THE MASKS '''\n",
    "masked_text = masked_tokens_data['masked text'].to_list()\n",
    "\n",
    "masked_text[1000:1010]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c97f9cf",
   "metadata": {},
   "source": [
    "### 3.3 Replacing Random Tokens with Text Distributions\n",
    "\n",
    "#### 3.3.1 Identifying the distibution of the Corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e4a6fffd-93ff-4284-8151-c2404af6a47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' REPLACING TOKENS WITH RESPECT TO THE POSITIVE OR NEGATIVE WORDS '''\n",
    "\n",
    "## Positive Words\n",
    "# Open and read a text file\n",
    "positive_path = r\"C:\\Users\\natal\\OneDrive\\university\\csci 5832\\project\\twitrratr positive words.txt\"\n",
    "\n",
    "with open(positive_path, 'r', encoding='utf-8') as file:\n",
    "    positive_words = file.read()\n",
    "    positive_words = positive_words.split(\"\\n\")\n",
    "\n",
    "negative_path = r\"C:\\Users\\natal\\OneDrive\\university\\csci 5832\\project\\twitrrater negative words.txt\"\n",
    "\n",
    "with open(negative_path, 'r', encoding='utf-8') as file:\n",
    "    negative_words = file.read()\n",
    "    negative_words = negative_words.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cd4026ba-3692-4e5e-9fc2-8ac92dc8e586",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = positive_words + negative_words\n",
    "word_list = list(set(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "535e690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' COMBINING THE ABOVE FUNCTIONS WITH THE MASKER '''\n",
    "\n",
    "## Creating a RegEx match pattern to split on whitespace\n",
    "word_pattern = r\"\\S+\"\n",
    "def random_masker(x):\n",
    "    ## Using RegEx findall to generate a list of the words from the tweet\n",
    "    words = re.findall(word_pattern,x)\n",
    "    \n",
    "    tweet_length = len(words)\n",
    "    ## Selecting a random token to mask from the length of the tweet\n",
    "    token_number = random.sample(range(tweet_length),1)\n",
    "    \n",
    "    random_num = random.randint(0, len(word_list))\n",
    "\n",
    "    words[token_number[0]] = word_list[random_num]\n",
    "    \n",
    "    ## Rejoining the list to form a tweet\n",
    "    tweet = ' '.join(words)\n",
    "    \n",
    "    return (tweet)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "616504fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dissertation meeting today - also handing application forms in. Dunno if Kelvingrove is an option today '"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mask = random_tokens_data.at[0, 'text']\n",
    "test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "717844e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dissertation meeting today - also handing application forms in. Dunno smiling Kelvingrove is an option today'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_masker(test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8c258ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tokens_data['masked text'] = random_tokens_data['text'].apply(lambda x: masker(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b882086e",
   "metadata": {},
   "source": [
    "### 3.   Combining + Saving the Masked Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "14de6706",
   "metadata": {},
   "outputs": [],
   "source": [
    "unmasked_data['masked text'] = unmasked_data['text']\n",
    "\n",
    "masked_corpus = pd.concat([unmasked_data,masked_tokens_data,random_tokens_data])\n",
    "masked_corpus.to_csv(\"Masked Sentiment140 Dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e233b77",
   "metadata": {},
   "source": [
    "### 4. Padded Version of the Data\n",
    "\n",
    "#### 4.1 Tokenizing the Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "24c1e39b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment label</th>\n",
       "      <th>masked text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>[USER] [URL] - Aww, that's a bummer.  You shou...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[USER] [URL] - Aww, that's a bummer.  You shou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>[USER] I dived many times for the ball. Manage...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[USER] I dived many times for the ball. Manage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>Negative</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>[USER] no, it's not behaving at all. i'm mad. ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[USER] no, it's not behaving at all. i'm mad. ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment    tweet_id                          date      flag  \\\n",
       "0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2          0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3          0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4          0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \\\n",
       "0  _TheSpecialOne_  [USER] [URL] - Aww, that's a bummer.  You shou...   \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...   \n",
       "2         mattycus  [USER] I dived many times for the ball. Manage...   \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire    \n",
       "4           Karoli  [USER] no, it's not behaving at all. i'm mad. ...   \n",
       "\n",
       "  sentiment label                                        masked text  \n",
       "0        Negative  [USER] [URL] - Aww, that's a bummer.  You shou...  \n",
       "1        Negative  is upset that he can't update his Facebook by ...  \n",
       "2        Negative  [USER] I dived many times for the ball. Manage...  \n",
       "3        Negative    my whole body feels itchy and like its on fire   \n",
       "4        Negative  [USER] no, it's not behaving at all. i'm mad. ...  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "711791cb-35ae-440e-ae2b-1e69c7a072cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_processer(text):\n",
    "    ## first, converting the string into a list format\n",
    "    text_ = text.split()\n",
    "    \n",
    "    text_storage = []\n",
    "    ## using the wordnet lemmatizer to lemmatize the words\n",
    "    for word in text_:\n",
    "        lemma = lemmatizer.lemmatize(word)\n",
    "        text_storage.append(lemma)\n",
    "        \n",
    "    ## removing all of the punctuation, special characters, digits, and trailing spaces using RegEx\n",
    "    text_for_cleaning = ' '.join(text_storage)\n",
    "    clean_text = re.sub('[!@#$%^&*()_+\\'\",.?*-+:;<>~`0-9]',' ',text_for_cleaning)\n",
    "    stripped_text = clean_text.strip()\n",
    "    \n",
    "    ##splitting the string back into a list \n",
    "    preprocessed_text = stripped_text.split()\n",
    "    \n",
    "    ## returning the the final processed text\n",
    "    return (preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f6cfaee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_corpus['listed text'] = masked_corpus['masked text'].apply (lambda x: sequence_processer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cb03c216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 388063\n"
     ]
    }
   ],
   "source": [
    "''' INDEXING THE TOKENS '''\n",
    "# Make a token2index dictionary and a index2token dictionary and convert the documents to sequences of indices\n",
    "token2index = {}\n",
    "index2token = {}\n",
    "saved_tokens = []\n",
    "index = 1 # reserve 0 for padding\n",
    "for document in masked_corpus['listed text'].to_list():\n",
    "    for token in document:\n",
    "\n",
    "        ## checking to see if the token is currently in the list of keys \n",
    "        if token in saved_tokens:\n",
    "        ## if it is pass\n",
    "            pass\n",
    "        \n",
    "        ## if the token is not currently in the list of keys\n",
    "        if token not in saved_tokens:\n",
    "            ## saving the token to iterate through\n",
    "            saved_tokens.append(token)\n",
    "            ## create a new key in the dictionary and adding the index\n",
    "            token2index[token] = index\n",
    "            ## adding one to the index\n",
    "            index = index + 1\n",
    "            \n",
    "## once the disctionary has been generated, converting the token2index to the index2token dictionary\n",
    "index2token = {k:v for k,v in zip(token2index.values(),token2index.keys())}\n",
    "\n",
    "token2index['[PAD]'] = 0\n",
    "index2token[0] = '[PAD]'\n",
    "\n",
    "print(f'Number of unique tokens: {len(token2index)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1a2e3df3-49f9-44c0-a379-800e84a97dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataset into sequences of indices\n",
    "def document_to_sequence(document : str) -> list:\n",
    "    return [token2index[token] for token in document]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6b49fdd6-6301-43f1-aa51-c4b4317e422f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...\n",
      "1    [20, 21, 5, 22, 23, 24, 25, 26, 27, 28, 29, 19...\n",
      "2    [1, 38, 39, 40, 41, 42, 43, 44, 45, 17, 46, 47...\n",
      "3             [52, 53, 54, 55, 56, 30, 57, 19, 58, 59]\n",
      "4    [1, 60, 19, 6, 61, 62, 63, 64, 65, 66, 67, 68,...\n",
      "Name: listed text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "sequences = masked_corpus['listed text'].apply(document_to_sequence)\n",
    "print(sequences.head()) # should now be a list of indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28ff161-dbfa-44d2-b385-fda79de0e3b5",
   "metadata": {},
   "source": [
    "#### 4.2 Padding the Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "20384ccc-fe95-4fab-8760-1dbb84a6cbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate the sequences\n",
    "def pad_sequence(sequence: list, max_length: int, padding_token: int = 0) -> list:\n",
    "    ## checking to see the length of the token \n",
    "    sequence_length = len(sequence)\n",
    "    #print (sequence)\n",
    "    #print (sequence_length)\n",
    "    \n",
    "    ## creating an if / else statement to check if it is above or below with the length.\n",
    "    if sequence_length >= max_length:\n",
    "        ## if the sequence length is above, it is truncated and only the first n for the max sequence are returned\n",
    "        truncated_sequence = sequence[0:max_length]\n",
    "        \n",
    "        return (truncated_sequence)\n",
    "\n",
    "    ## else: the pad token is appended onto the end until it is the max sequence\n",
    "    else:\n",
    "        \n",
    "        ## finds the difference between the length of the sequence and the max length\n",
    "        difference = max_length - sequence_length\n",
    " \n",
    "        # generating a list with the length of the difference\n",
    "        padded_list = ['[PAD]'] * difference\n",
    "   \n",
    "        ## extending the sequence with the pad values\n",
    "        padded_sequence = sequence + padded_list\n",
    "        ## returning the list\n",
    "        return (padded_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0a44b44f-2877-417b-9fd9-af9623aab9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...\n",
      "1    [20, 21, 5, 22, 23, 24, 25, 26, 27, 28, 29, 19...\n",
      "2    [1, 38, 39, 40, 41, 42, 43, 44, 45, 17, 46, 47...\n",
      "3    [52, 53, 54, 55, 56, 30, 57, 19, 58, 59, [PAD]...\n",
      "4    [1, 60, 19, 6, 61, 62, 63, 64, 65, 66, 67, 68,...\n",
      "Name: listed text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Maximum sequence length\n",
    "max_length = 40\n",
    "\n",
    "# Truncate the sequences\n",
    "truncated_sequences = sequences.apply(lambda x: pad_sequence(x, max_length))\n",
    "\n",
    "print(truncated_sequences.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6a683d0c-a989-4b6e-99ac-63c29f764820",
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_sequences.reset_index(inplace=True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3eae9cfc-8315-40fd-a69f-20d24a003fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences.reset_index(inplace=True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a2320db7-efc4-4412-8e51-e32c0d784bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_corpus.reset_index(inplace=True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "702bb5fa-adae-4011-9309-9ce6d34d3a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_corpus['truncated sequences'] = truncated_sequences\n",
    "masked_corpus['tokenized sequences'] = sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "49bc08bc-b16c-4cd4-8dc0-650786898445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[USER]', 'how', '[MASK]', 'doin']\n",
      "[1, 88, 353990, 9618, '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "[1, 88, 353990, 9618, '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "print(masked_corpus.at[1649977, 'listed text'])\n",
    "print(masked_corpus.at[1649977, 'truncated sequences'])\n",
    "print(masked_corpus.at[1649977, 'tokenized sequences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0dbd00da-546c-436c-8a50-b46eaa4a0fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_corpus.to_csv('Cleaned Sentiment140 Dataset with Tokenized Sequences.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c26c94f5-be2c-4dad-ad13-d60aba4b9305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment label</th>\n",
       "      <th>masked text</th>\n",
       "      <th>listed text</th>\n",
       "      <th>truncated sequences</th>\n",
       "      <th>tokenized sequences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>[USER] [URL] - Aww, that's a bummer.  You shou...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[USER] [URL] - Aww, that's a bummer.  You shou...</td>\n",
       "      <td>[[USER], [URL], -, Aww, that, s, a, bummer, Yo...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>[is, upset, that, he, can, t, update, his, Fac...</td>\n",
       "      <td>[20, 21, 5, 22, 23, 24, 25, 26, 27, 28, 29, 19...</td>\n",
       "      <td>[20, 21, 5, 22, 23, 24, 25, 26, 27, 28, 29, 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>[USER] I dived many times for the ball. Manage...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[USER] I dived many times for the ball. Manage...</td>\n",
       "      <td>[[USER], I, dived, many, time, for, the, ball,...</td>\n",
       "      <td>[1, 38, 39, 40, 41, 42, 43, 44, 45, 17, 46, 47...</td>\n",
       "      <td>[1, 38, 39, 40, 41, 42, 43, 44, 45, 17, 46, 47...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>Negative</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[my, whole, body, feel, itchy, and, like, it, ...</td>\n",
       "      <td>[52, 53, 54, 55, 56, 30, 57, 19, 58, 59, [PAD]...</td>\n",
       "      <td>[52, 53, 54, 55, 56, 30, 57, 19, 58, 59, [PAD]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>[USER] no, it's not behaving at all. i'm mad. ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[USER] no, it's not behaving at all. i'm mad. ...</td>\n",
       "      <td>[[USER], no, it, s, not, behaving, at, all, i,...</td>\n",
       "      <td>[1, 60, 19, 6, 61, 62, 63, 64, 65, 66, 67, 68,...</td>\n",
       "      <td>[1, 60, 19, 6, 61, 62, 63, 64, 65, 66, 67, 68,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1649973</th>\n",
       "      <td>0</td>\n",
       "      <td>2175233627</td>\n",
       "      <td>Mon Jun 15 00:26:02 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>babbittry</td>\n",
       "      <td>Can't switch to sprint....no palm pre...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Can't [MASK] to sprint....no palm pre...</td>\n",
       "      <td>[Can, t, [MASK], to, sprint, no, palm, pre]</td>\n",
       "      <td>[698, 24, 353990, 17, 869, 60, 9068, 2100, [PA...</td>\n",
       "      <td>[698, 24, 353990, 17, 869, 60, 9068, 2100, [PA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1649974</th>\n",
       "      <td>4</td>\n",
       "      <td>2070543008</td>\n",
       "      <td>Sun Jun 07 17:52:58 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>jeremyrbennett</td>\n",
       "      <td>[USER] you should be at youth group!!!!    --R...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[USER] you should be at [MASK] group!!!! --Roc...</td>\n",
       "      <td>[[USER], you, should, be, at, [MASK], group, -...</td>\n",
       "      <td>[1, 73, 557, 418, 63, 353990, 2950, 388061, 38...</td>\n",
       "      <td>[1, 73, 557, 418, 63, 353990, 2950, 388061, 38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1649975</th>\n",
       "      <td>4</td>\n",
       "      <td>1974202139</td>\n",
       "      <td>Sat May 30 12:38:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Pwiimus</td>\n",
       "      <td>[USER] Stavros Flatly, you know it makes sense</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[USER] [MASK] Flatly, you know it makes sense</td>\n",
       "      <td>[[USER], [MASK], Flatly, you, know, it, make, ...</td>\n",
       "      <td>[1, 353990, 90627, 73, 269, 19, 214, 3669, [PA...</td>\n",
       "      <td>[1, 353990, 90627, 73, 269, 19, 214, 3669, [PA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1649976</th>\n",
       "      <td>4</td>\n",
       "      <td>2054221301</td>\n",
       "      <td>Sat Jun 06 06:51:36 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>marshymiffy</td>\n",
       "      <td>[USER] Aww! you're welcome!</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[USER] Aww! [MASK] welcome!</td>\n",
       "      <td>[[USER], Aww, [MASK], welcome]</td>\n",
       "      <td>[1, 4, 353990, 5898, [PAD], [PAD], [PAD], [PAD...</td>\n",
       "      <td>[1, 4, 353990, 5898, [PAD], [PAD], [PAD], [PAD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1649977</th>\n",
       "      <td>4</td>\n",
       "      <td>1676396766</td>\n",
       "      <td>Fri May 01 21:55:37 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>missnacil</td>\n",
       "      <td>[USER] how u doin</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[USER] how [MASK] doin</td>\n",
       "      <td>[[USER], how, [MASK], doin]</td>\n",
       "      <td>[1, 88, 353990, 9618, [PAD], [PAD], [PAD], [PA...</td>\n",
       "      <td>[1, 88, 353990, 9618, [PAD], [PAD], [PAD], [PA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1649978 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment    tweet_id                          date      flag  \\\n",
       "0                0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1                0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2                0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3                0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4                0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "...            ...         ...                           ...       ...   \n",
       "1649973          0  2175233627  Mon Jun 15 00:26:02 PDT 2009  NO_QUERY   \n",
       "1649974          4  2070543008  Sun Jun 07 17:52:58 PDT 2009  NO_QUERY   \n",
       "1649975          4  1974202139  Sat May 30 12:38:50 PDT 2009  NO_QUERY   \n",
       "1649976          4  2054221301  Sat Jun 06 06:51:36 PDT 2009  NO_QUERY   \n",
       "1649977          4  1676396766  Fri May 01 21:55:37 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    user                                               text  \\\n",
       "0        _TheSpecialOne_  [USER] [URL] - Aww, that's a bummer.  You shou...   \n",
       "1          scotthamilton  is upset that he can't update his Facebook by ...   \n",
       "2               mattycus  [USER] I dived many times for the ball. Manage...   \n",
       "3                ElleCTF    my whole body feels itchy and like its on fire    \n",
       "4                 Karoli  [USER] no, it's not behaving at all. i'm mad. ...   \n",
       "...                  ...                                                ...   \n",
       "1649973        babbittry          Can't switch to sprint....no palm pre...    \n",
       "1649974   jeremyrbennett  [USER] you should be at youth group!!!!    --R...   \n",
       "1649975          Pwiimus    [USER] Stavros Flatly, you know it makes sense    \n",
       "1649976      marshymiffy                       [USER] Aww! you're welcome!    \n",
       "1649977        missnacil                                  [USER] how u doin   \n",
       "\n",
       "        sentiment label                                        masked text  \\\n",
       "0              Negative  [USER] [URL] - Aww, that's a bummer.  You shou...   \n",
       "1              Negative  is upset that he can't update his Facebook by ...   \n",
       "2              Negative  [USER] I dived many times for the ball. Manage...   \n",
       "3              Negative    my whole body feels itchy and like its on fire    \n",
       "4              Negative  [USER] no, it's not behaving at all. i'm mad. ...   \n",
       "...                 ...                                                ...   \n",
       "1649973        Negative           Can't [MASK] to sprint....no palm pre...   \n",
       "1649974        Positive  [USER] you should be at [MASK] group!!!! --Roc...   \n",
       "1649975        Positive      [USER] [MASK] Flatly, you know it makes sense   \n",
       "1649976        Positive                        [USER] Aww! [MASK] welcome!   \n",
       "1649977        Positive                             [USER] how [MASK] doin   \n",
       "\n",
       "                                               listed text  \\\n",
       "0        [[USER], [URL], -, Aww, that, s, a, bummer, Yo...   \n",
       "1        [is, upset, that, he, can, t, update, his, Fac...   \n",
       "2        [[USER], I, dived, many, time, for, the, ball,...   \n",
       "3        [my, whole, body, feel, itchy, and, like, it, ...   \n",
       "4        [[USER], no, it, s, not, behaving, at, all, i,...   \n",
       "...                                                    ...   \n",
       "1649973        [Can, t, [MASK], to, sprint, no, palm, pre]   \n",
       "1649974  [[USER], you, should, be, at, [MASK], group, -...   \n",
       "1649975  [[USER], [MASK], Flatly, you, know, it, make, ...   \n",
       "1649976                     [[USER], Aww, [MASK], welcome]   \n",
       "1649977                        [[USER], how, [MASK], doin]   \n",
       "\n",
       "                                       truncated sequences  \\\n",
       "0        [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "1        [20, 21, 5, 22, 23, 24, 25, 26, 27, 28, 29, 19...   \n",
       "2        [1, 38, 39, 40, 41, 42, 43, 44, 45, 17, 46, 47...   \n",
       "3        [52, 53, 54, 55, 56, 30, 57, 19, 58, 59, [PAD]...   \n",
       "4        [1, 60, 19, 6, 61, 62, 63, 64, 65, 66, 67, 68,...   \n",
       "...                                                    ...   \n",
       "1649973  [698, 24, 353990, 17, 869, 60, 9068, 2100, [PA...   \n",
       "1649974  [1, 73, 557, 418, 63, 353990, 2950, 388061, 38...   \n",
       "1649975  [1, 353990, 90627, 73, 269, 19, 214, 3669, [PA...   \n",
       "1649976  [1, 4, 353990, 5898, [PAD], [PAD], [PAD], [PAD...   \n",
       "1649977  [1, 88, 353990, 9618, [PAD], [PAD], [PAD], [PA...   \n",
       "\n",
       "                                       tokenized sequences  \n",
       "0        [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...  \n",
       "1        [20, 21, 5, 22, 23, 24, 25, 26, 27, 28, 29, 19...  \n",
       "2        [1, 38, 39, 40, 41, 42, 43, 44, 45, 17, 46, 47...  \n",
       "3        [52, 53, 54, 55, 56, 30, 57, 19, 58, 59, [PAD]...  \n",
       "4        [1, 60, 19, 6, 61, 62, 63, 64, 65, 66, 67, 68,...  \n",
       "...                                                    ...  \n",
       "1649973  [698, 24, 353990, 17, 869, 60, 9068, 2100, [PA...  \n",
       "1649974  [1, 73, 557, 418, 63, 353990, 2950, 388061, 38...  \n",
       "1649975  [1, 353990, 90627, 73, 269, 19, 214, 3669, [PA...  \n",
       "1649976  [1, 4, 353990, 5898, [PAD], [PAD], [PAD], [PAD...  \n",
       "1649977  [1, 88, 353990, 9618, [PAD], [PAD], [PAD], [PA...  \n",
       "\n",
       "[1649978 rows x 11 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5959412",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Bibliography\n",
    "\n",
    "Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (No. arXiv:1810.04805). arXiv. http://arxiv.org/abs/1810.04805\n",
    "\n",
    "\n",
    "Go, A., Bhayani, R., & Huang, L. (2009). Twitter Sentiment Classiﬁcation using Distant Supervision. CS224N Project Report.\n",
    "\n",
    "Jurafsky, D., & Martin, J. (2025). Speech and Language Processing (3rd ed.). https://web.stanford.edu/~jurafsky/slp3/ed3book_Jan25.pdf\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
